{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dEKhPSO82y85",
   "metadata": {
    "id": "dEKhPSO82y85"
   },
   "source": [
    "## 0. Configuartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "worthy-resolution",
   "metadata": {
    "id": "worthy-resolution"
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "import os\n",
    "import sys\n",
    "\n",
    "device = 'cuda'\n",
    "wait_for_v100 = False\n",
    "ds_to_use = [\n",
    "    # 'HP1-FvM',\n",
    "    # 'HP1-RB',\n",
    "    # 'HP2-FvM',\n",
    "    # 'HP2-RB',\n",
    "    # 'HP3-FvM',\n",
    "    # 'HP3-RB',\n",
    "    # 'HP4-FvM',\n",
    "    'HP4-RB',\n",
    "    # 'HP5-FvM',\n",
    "    # 'HP5-RB',\n",
    "    # 'HP6-FvM',\n",
    "    # 'HP6-RB',\n",
    "    # 'HP7-FvM',\n",
    "    # 'HP7-RB'\n",
    "]\n",
    "home = os.getenv(\"HOME\")\n",
    "use_gdrive = False\n",
    "trained_model_directory = 'trained_model'\n",
    "# os.environ['WANDB_API_KEY'] = '6cec6a71175dec59b32a9067a0664dec76208f2a'\n",
    "# os.environ['WANDB_NAME'] = 'GermanWaveToVec Training'\n",
    "# os.environ['WANDB_ENTITY'] ='wandb.ai/elunrast'\n",
    "# os.environ['WANDB_PROJECT'] ='GermanWaveToVec'\n",
    "\n",
    "## ----- Training arguments  ----- ##\n",
    "per_device_train_batch_size = 4\n",
    "# TODO: aus freiem Grafikkarten Speicher ableiten (Graphikkarte mit 11GB)\n",
    "per_device_eval_batch_size  = per_device_train_batch_size // 2\n",
    "gradient_accumulation_steps = 2\n",
    "logging_steps               = 100\n",
    "max_steps                   = per_device_train_batch_size * logging_steps\n",
    "max_trainingset_size        = max_steps * per_device_train_batch_size # TODO freiem Grafikkarten Speicher ableiten\n",
    "max_steps                   = max_trainingset_size // per_device_train_batch_size # Schritte pro Epoche\n",
    "warmup_steps                = max_steps // 4\n",
    "save_steps                  = max_steps // 2\n",
    "eval_steps                  = save_steps\n",
    "max_sample_size             = 1200   # None -> no restriction\n",
    "max_trainingset_size        = per_device_train_batch_size * max_steps   # None -> no restriction\n",
    "fixed_training_set_size     = 100 * per_device_eval_batch_size          # None -> 20% der Originaldatensatz Größe \n",
    "num_train_epochs            = 3  # Anzahl Durchläufe für ein Dataset\n",
    "max_rounds                  = 4  # Anzahl Trainingsrunden über alle used_datasets\n",
    "\n",
    "learning_rate = per_device_train_batch_size * ((3e-4)/16)  # 3e-4/4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-budapest",
   "metadata": {
    "id": "motivated-budapest"
   },
   "source": [
    "## 1. Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stuffed-silicon",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48755,
     "status": "ok",
     "timestamp": 1620812177354,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "stuffed-silicon",
    "outputId": "022a280e-ada5-49f2-fe80-2bd02775ed95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs on colab: False\n"
     ]
    }
   ],
   "source": [
    "local_nlp_base_dir = f'{home}/shared'\n",
    "local_data_base_dir = f'{local_nlp_base_dir}/NLP-Data'\n",
    "local_data_dir = f'{local_data_base_dir}/audio'\n",
    "extern_nlp_base_dir = None\n",
    "extern_data_base_dir = None\n",
    "extern_data_dir = None\n",
    "\n",
    "runs_on_colab = (home == '/root')\n",
    "print( f'runs on colab: {runs_on_colab}')\n",
    "\n",
    "if not runs_on_colab:\n",
    "    os.environ['http_proxy'] = 'http://192.168.8.50:3128'\n",
    "    os.environ['https_proxy'] = 'http://192.168.8.50:3128'\n",
    "else:\n",
    "    # to get access to the datasets we use gdrive\n",
    "    use_gdrive = True\n",
    "    # install packages\n",
    "    !pip install datasets==1.4.1\n",
    "    !pip install transformers==4.4.0\n",
    "    !pip install jiwer\n",
    "    !pip install torchaudio\n",
    "    !pip install librosa\n",
    "    # create local directories\n",
    "    !mkdir $local_nlp_base_dir\n",
    "    !mkdir $local_data_base_dir\n",
    "\n",
    "if use_gdrive:\n",
    "    gdrive_base = '/content/gdrive'    \n",
    "    extern_nlp_base_dir = f'{gdrive_base}/MyDrive'\n",
    "    extern_data_base_dir = f'{extern_nlp_base_dir}/NLP-Data'\n",
    "    extern_data_dir = f'{extern_data_base_dir}/audio'\n",
    "\n",
    "    if not os.path.isdir(gdrive_base):\n",
    "        from google.colab import drive\n",
    "        drive.mount(gdrive_base)\n",
    "\n",
    "if not os.path.isdir(local_data_dir):\n",
    "    !mkdir $local_data_dir\n",
    "\n",
    "if extern_nlp_base_dir:\n",
    "    model_dir = f'{extern_nlp_base_dir}/NLP-Models/GermanWave2Vec'\n",
    "else:\n",
    "    model_dir = f'{local_nlp_base_dir}/NLP-Models/GermanWave2Vec'\n",
    "\n",
    "# Use the Model from this Directory (Base: .../NLP-Models/GermanWave2Vec/)\n",
    "# None -> Start from 'facebook/wav2vec2-large-xlsr-53-german'\n",
    "trained_model_path = f'{model_dir}/{trained_model_directory}'\n",
    "\n",
    "git_views_dir = f'{local_nlp_base_dir}/gitviews/extern'\n",
    "git_view_path = f'{git_views_dir}/GermanWave2Vec'\n",
    "\n",
    "if not os.path.isdir(git_views_dir):\n",
    "    !mkdir $git_views_dir\n",
    "    !cd $git_views_dir; git clone https://github.com/ElUnrast/GermanWave2Vec.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4wLJpgjxOFRM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1773,
     "status": "ok",
     "timestamp": 1620812249370,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "4wLJpgjxOFRM",
    "outputId": "48670cda-fe17-40f7-d7ad-967f07b28354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script-Path: /home/ki-mo/shared/gitviews/extern/GermanWave2Vec/python\n"
     ]
    }
   ],
   "source": [
    "if runs_on_colab:\n",
    "    !cd $git_view_path; git fetch --all; git reset --hard origin/main\n",
    "\n",
    "script_path = f'{git_views_dir}/GermanWave2Vec/python'\n",
    "print(f'Script-Path: {script_path}')\n",
    "sys.path.insert(0, script_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-footwear",
   "metadata": {
    "id": "better-footwear"
   },
   "source": [
    "## 2. Check Runtime Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accompanied-wright",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 591,
     "status": "ok",
     "timestamp": 1620812267985,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "accompanied-wright",
    "outputId": "a6984eb8-cb57-4fa0-8dae-f817850837ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 17 13:52:16 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 37%   40C    P8    13W / 260W |     91MiB / 11016MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1134      G   /usr/lib/xorg/Xorg                 26MiB |\n",
      "|    0   N/A  N/A      1236      G   /usr/bin/gnome-shell               62MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "if 'cuda' == device:\n",
    "    gpu_info = !nvidia-smi\n",
    "    gpu_info = '\\n'.join(gpu_info)\n",
    "\n",
    "    if runs_on_colab:\n",
    "        if wait_for_v100 and not gpu_info.find('V100') >= 0:\n",
    "            print('The current GPU is not a V100')\n",
    "            print('Since you want to wait for a V100 the current session is aborted')\n",
    "            raise ValueError\n",
    "\n",
    "        if gpu_info.find('failed') >= 0:\n",
    "            print('For training, please use a VM with GPU!')\n",
    "            raise ValueError\n",
    "            \n",
    "        from psutil import virtual_memory\n",
    "        ram_gb = virtual_memory().total / 1e9\n",
    "        print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "        if ram_gb < 20:\n",
    "            print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
    "            print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
    "            print('re-execute this cell.')\n",
    "        else:\n",
    "            print('You are using a high-RAM runtime!')\n",
    "            \n",
    "    print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-liver",
   "metadata": {
    "id": "connected-liver"
   },
   "source": [
    "## 3 Install packages and do Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "copyrighted-peeing",
   "metadata": {
    "id": "copyrighted-peeing"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "from transformers.trainer_pt_utils import LengthGroupedSampler, DistributedLengthGroupedSampler\n",
    "\n",
    "import json\n",
    "import collections\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import jiwer\n",
    "from jiwer import wer\n",
    "from datasets import load_metric\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7desxN6tZVU3",
   "metadata": {
    "id": "7desxN6tZVU3"
   },
   "outputs": [],
   "source": [
    "# %aimport SnippetDatasets\n",
    "# %aimport GermanSpeechDatasetWidgetFactory\n",
    "# %aimport GermanSpeechToTextTranslater\n",
    "# %autoreload 2\n",
    "from SnippetDatasets import SnippetDatasets\n",
    "from GermanSpeechDatasetWidgetFactory import GermanSpeechDatasetWidgetFactory\n",
    "from GermanSpeechToTextTranslater import GermanSpeechToTextTranslater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-singing",
   "metadata": {
    "id": "certified-singing"
   },
   "source": [
    "## 4 Initialize Helper Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unexpected-ideal",
   "metadata": {
    "id": "unexpected-ideal",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_datasets = SnippetDatasets(\n",
    "    runs_on_colab, \n",
    "    local_audio_base_dir=local_data_dir, \n",
    "    git_repository=git_view_path,\n",
    "    extern_audio_base_dir=extern_data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "measured-craps",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "7b46e89a13764745915b7802aaedc5ee",
      "7d8e46c71b3e45cfa775157a66ac5a52",
      "78f2db221c504bfeb80d08123dde7808",
      "03c3d21fb26343a8b6fec43119e854ce",
      "449bce20528d4f0fa84a533edf31868f",
      "f6212268701e4b5485254734029c7eda",
      "da0a96e9b4194da4b5b242dc2d716bd9",
      "03ea4a943dfa478e9a1a87e251c05af6"
     ]
    },
    "executionInfo": {
     "elapsed": 77594,
     "status": "ok",
     "timestamp": 1620812368354,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "measured-craps",
    "outputId": "e7a3968e-7107-4634-eab1-9ec5a4a2e424",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Model from Path: /home/ki-mo/shared/NLP-Models/GermanWave2Vec/trained_model\n",
      "Using Model: /home/ki-mo/shared/NLP-Models/GermanWave2Vec/trained_model\n",
      "Loading processor\n",
      "Loading metric\n",
      "json loaded: {'trained_epochs': 4}\n",
      "Saved Epoch: 4\n",
      "Loading model. Epoche 4\n"
     ]
    }
   ],
   "source": [
    "model_path = None\n",
    "checkpoint = None\n",
    "\n",
    "if checkpoint:\n",
    "    model_path = f'{trained_model_path}/{checkpoint}'\n",
    "elif os.path.isfile(f'{trained_model_path}/pytorch_model.bin'):\n",
    "    model_path = trained_model_path\n",
    "\n",
    "print(f'Initialize Model from Path: {model_path}')\n",
    "translator = GermanSpeechToTextTranslater(model_name=model_path, ds_handler=my_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XHvPPiWru_IC",
   "metadata": {
    "id": "XHvPPiWru_IC"
   },
   "source": [
    "## 4. Choose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "A2FODbnsFmoC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 890,
     "status": "error",
     "timestamp": 1620852931702,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "A2FODbnsFmoC",
    "outputId": "ce71c4b2-5ec1-41e9-a7cb-6ba14db9178d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please choose Datasets to use\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a570379d9af4227bf6904a26919931f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Checkbox(value=False, description='HP1-FvM', indent=False), Checkbox(value=False, description='H…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget_factory = GermanSpeechDatasetWidgetFactory(my_datasets, ds_to_use)\n",
    "print('Please choose Datasets to use')\n",
    "ds_checkboxes_widget = widget_factory.create_dataset_choice_widget()\n",
    "display(ds_checkboxes_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "governmental-found",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================\n",
      "Starting round 0 of 4\n",
      "-----------------------------\n",
      "Loading Dataset: HP4-RB - content-translated-with_original.csv\n",
      "Pruning Dataset HP4-RB with 15499 Entries\n",
      " - 15499 Entries left after Length Cut (min=31, max=4000)\n",
      " - 15499 Entries left after Action Cut\n",
      "Dataset - HP4-RB loaded with 15499 Entries\n",
      "**************************************\n",
      "Starting round 0 of 4, epoche 0 of 3\n",
      "Splitting Dataset HP4-RB with 15499 Entries\n",
      "aktual trained epoches: 4\n",
      "old trained epoches: 4\n",
      "Actual number of bad translated 5287\n",
      "Actual WER: 5.537159475856961\n",
      " - 2500 left after Entries Max Samples Cut (max=2500)\n",
      "Creating Trainer for HP4-RB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training of Dataset: HP4-RB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 26:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model\n",
      "**************************************\n",
      "Starting round 0 of 4, epoche 1 of 3\n",
      "Splitting Dataset HP4-RB with 15499 Entries\n",
      "aktual trained epoches: 5\n",
      "old trained epoches: 4\n",
      "Translate all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd9665ad02944d08b392d39594ed82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15499.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki-mo/python_env/qca38/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculate WER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 0.14232574858126287\n",
      "No. of bad translated snippets: 9949\n",
      "Saving diff files\n",
      "Actual number of bad translated 9949\n",
      "Actual WER: 0.14232574858126287\n",
      " - 2500 left after Entries Max Samples Cut (max=2500)\n",
      "Creating Trainer for HP4-RB\n",
      "Training of Dataset: HP4-RB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 20:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Model\n",
      "**************************************\n",
      "Starting round 0 of 4, epoche 2 of 3\n",
      "Splitting Dataset HP4-RB with 15499 Entries\n",
      "aktual trained epoches: 6\n",
      "old trained epoches: 5\n",
      "Translate all\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f3607ac4f24d2ea9f03a4afd2c794c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15499.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki-mo/python_env/qca38/lib/python3.8/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 10.76 GiB total capacity; 6.40 GiB already allocated; 1.36 GiB free; 7.83 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c1e4df16c883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m translator.train(\n\u001b[0m\u001b[1;32m      2\u001b[0m         \u001b[0mtrained_model_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrained_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mdataset_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_datasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mds_to_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidget_factory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_used_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_trainingset_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_trainingset_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shared/gitviews/extern/GermanWave2Vec/python/GermanSpeechToTextTranslater.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trained_model_path, dataset_loader, ds_to_train, max_trainingset_size, max_rounds, num_train_epochs, num_steps_per_epoche, per_device_train_batch_size, gradient_accumulation_steps, logging_steps, learning_rate, warmup_steps, early_stopping_value)\u001b[0m\n\u001b[1;32m    427\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Starting round {runde} of {max_rounds}, epoche {epoche} of {num_train_epochs}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Splitting Dataset {ds_id} with {pandas_df.shape[0]} Entries'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m                     \u001b[0mbad_translation_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwer_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Actual number of bad translated {bad_translation_ds.shape[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Actual WER: {wer_result}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shared/gitviews/extern/GermanWave2Vec/python/GermanSpeechToTextTranslater.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, ds_id, pandas_df, diff_file_extension, diff_calc_wer)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Translate all'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp3_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtranslation_column_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ModelEpoche'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shared/gitviews/extern/GermanWave2Vec/python/GermanSpeechToTextTranslater.py\u001b[0m in \u001b[0;36mtranslate_dataset\u001b[0;34m(self, mp3_dir, ds)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'{mp3_dir}/{file_name}'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_audio_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_audio_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shared/gitviews/extern/GermanWave2Vec/python/GermanSpeechToTextTranslater.py\u001b[0m in \u001b[0;36mtranslate_audio_files\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mtranslated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mtranslated_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0msize_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shared/gitviews/extern/GermanWave2Vec/python/GermanSpeechToTextTranslater.py\u001b[0m in \u001b[0;36mtranslate_audio\u001b[0;34m(self, audio_file_name)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Storing predicted ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m         outputs = self.wav2vec2(\n\u001b[0m\u001b[1;32m   1046\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    870\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_feature_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    642\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                     layer_outputs = torch.utils.checkpoint.checkpoint(\n\u001b[0m\u001b[1;32m    645\u001b[0m                         \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unexpected keyword arguments: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mcustom_forward\u001b[0;34m(*inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_custom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m                         \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mattn_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         hidden_states, attn_weights, _ = self.attention(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         )\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer_head_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python_env/qca38/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1583\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1584\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 10.76 GiB total capacity; 6.40 GiB already allocated; 1.36 GiB free; 7.83 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "translator.train(\n",
    "        trained_model_path=trained_model_path,\n",
    "        dataset_loader=my_datasets, \n",
    "        ds_to_train=widget_factory.get_used_datasets(), \n",
    "        max_trainingset_size=max_trainingset_size, \n",
    "        max_rounds=max_rounds, \n",
    "        num_train_epochs=num_train_epochs,\n",
    "        num_steps_per_epoche=max_steps,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        logging_steps=logging_steps,\n",
    "        learning_rate=learning_rate,\n",
    "        warmup_steps=warmup_steps,\n",
    "        early_stopping_value=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rPHzCcxaYJAz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 569,
     "status": "ok",
     "timestamp": 1620853210989,
     "user": {
      "displayName": "Elius Tibelius",
      "photoUrl": "",
      "userId": "00452752923460326984"
     },
     "user_tz": -120
    },
    "id": "rPHzCcxaYJAz",
    "outputId": "541ea6e0-2a5a-4ed9-a7c8-73b876f1fe89"
   },
   "outputs": [],
   "source": [
    "print('finished')\n",
    "widget_factory.play_audio_file(audio_url=\"http://www.w3schools.com/html/horse.ogg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-adams",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "better-footwear"
   ],
   "name": "Wave2Vec-Train.ipynb",
   "provenance": [
    {
     "file_id": "18nAJ5oJgm56etRE3MTVNC4ofrVNHgEQG",
     "timestamp": 1621004295534
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03c3d21fb26343a8b6fec43119e854ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03ea4a943dfa478e9a1a87e251c05af6",
      "placeholder": "​",
      "style": "IPY_MODEL_da0a96e9b4194da4b5b242dc2d716bd9",
      "value": " 3.90k/? [00:00&lt;00:00, 15.9kB/s]"
     }
    },
    "03ea4a943dfa478e9a1a87e251c05af6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "449bce20528d4f0fa84a533edf31868f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "78f2db221c504bfeb80d08123dde7808": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6212268701e4b5485254734029c7eda",
      "max": 1764,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_449bce20528d4f0fa84a533edf31868f",
      "value": 1764
     }
    },
    "7b46e89a13764745915b7802aaedc5ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78f2db221c504bfeb80d08123dde7808",
       "IPY_MODEL_03c3d21fb26343a8b6fec43119e854ce"
      ],
      "layout": "IPY_MODEL_7d8e46c71b3e45cfa775157a66ac5a52"
     }
    },
    "7d8e46c71b3e45cfa775157a66ac5a52": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da0a96e9b4194da4b5b242dc2d716bd9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6212268701e4b5485254734029c7eda": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
