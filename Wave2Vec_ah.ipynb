{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "Wave2Vec_ah.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f9baed1d2f64d4492467605c4336ffc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "BoxModel",
          "state": {
            "_view_name": "BoxView",
            "_dom_classes": [],
            "_model_name": "BoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_72ce045d20aa4ddebeaeb65e5067b799",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_97e63375095b43b4a37e89b92646c69f",
              "IPY_MODEL_5021dcab8b324849b30b5f4571b0d525",
              "IPY_MODEL_93159f0b6985444e8c735261def3e951",
              "IPY_MODEL_f1de6a11546e4c01801941129cb32109",
              "IPY_MODEL_cd78cebf3e8e45feab11eb7476b0b51c",
              "IPY_MODEL_f8eee87f3cf84b2aa0ddd8159c8c0e77",
              "IPY_MODEL_f17e2066a1b743e88d2fe2d4f73db8bb"
            ]
          }
        },
        "72ce045d20aa4ddebeaeb65e5067b799": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97e63375095b43b4a37e89b92646c69f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_f7e01e3e02c54cdf9dda32133db638a3",
            "_dom_classes": [],
            "description": "HP1-FvM",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": true,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_86dc5c95694f436d86de86887f0fdc81"
          }
        },
        "5021dcab8b324849b30b5f4571b0d525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_e3ad19fff61149639b42ea1c6817e5f9",
            "_dom_classes": [],
            "description": "HP1-RB",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dc4906791f5a4575842b627b90c298b2"
          }
        },
        "93159f0b6985444e8c735261def3e951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_15ef26b6d3d2424d88278cafc7b90185",
            "_dom_classes": [],
            "description": "HP2-FvM",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d41f5c7909c46ba87ccb185b3756fb0"
          }
        },
        "f1de6a11546e4c01801941129cb32109": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_46ed4f6fe3d249879807afaf62f56d7d",
            "_dom_classes": [],
            "description": "HP3-FvM",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_acaadf06c3d54139acac2fead50f0acf"
          }
        },
        "cd78cebf3e8e45feab11eb7476b0b51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_0cc2d2a5c9354902ab7df80c2ccb810c",
            "_dom_classes": [],
            "description": "HP3-RB",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c75bd938352c499ab58fc7c553119d8e"
          }
        },
        "f8eee87f3cf84b2aa0ddd8159c8c0e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_94f6823155e34ef0907f4e6c97346ccd",
            "_dom_classes": [],
            "description": "HP4-FvM",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d186d3bfe18b44d68e060cacc959e076"
          }
        },
        "f17e2066a1b743e88d2fe2d4f73db8bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "state": {
            "_view_name": "CheckboxView",
            "style": "IPY_MODEL_97afa66d1c854a1b8fe1511a580b20cf",
            "_dom_classes": [],
            "description": "HP5-FvM",
            "_model_name": "CheckboxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": false,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "indent": false,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e02e1aac6dc46a58fc123cc4589187d"
          }
        },
        "f7e01e3e02c54cdf9dda32133db638a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "86dc5c95694f436d86de86887f0fdc81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3ad19fff61149639b42ea1c6817e5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dc4906791f5a4575842b627b90c298b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "15ef26b6d3d2424d88278cafc7b90185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d41f5c7909c46ba87ccb185b3756fb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "46ed4f6fe3d249879807afaf62f56d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "acaadf06c3d54139acac2fead50f0acf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0cc2d2a5c9354902ab7df80c2ccb810c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c75bd938352c499ab58fc7c553119d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94f6823155e34ef0907f4e6c97346ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d186d3bfe18b44d68e060cacc959e076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97afa66d1c854a1b8fe1511a580b20cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e02e1aac6dc46a58fc123cc4589187d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElUnrast/GermanWave2Vec/blob/main/Wave2Vec_ah.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEKhPSO82y85"
      },
      "source": [
        "## 0. Configuartion"
      ],
      "id": "dEKhPSO82y85"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worthy-resolution"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "device = 'cuda'\n",
        "wait_for_v100 = False\n",
        "ds_to_us = ['HP1-FvM']\n",
        "home = os.getenv(\"HOME\")\n",
        "use_gdrive = False\n",
        "trained_model_directory = 'trained_model_ah'\n",
        "WANDB_API_KEY='6cec6a71175dec59b32a9067a0664dec76208f2a'\n",
        "WANDB_NAME='GermanWaveToVec Training'\n",
        "WANDB_ENTITY='wandb.ai/elunrast'\n",
        "WANDB_PROJECT='GermanWaveToVec'\n",
        "# Use Model from Checkpoint. \n",
        "# The Value must be a Sub-Directory of {trained_model_path} (Example: 'checkpoint-3600')\n",
        "checkpoint = None\n",
        "\n",
        "## ----- Training arguments  ----- ##\n",
        "per_device_train_batch_size = 5      # TODO: aus freiem Grafikkarten Speicher ableiten (Graphikkarte mit 11GB)\n",
        "per_device_eval_batch_size  = per_device_train_batch_size // 2\n",
        "gradient_accumulation_steps = 2\n",
        "logging_steps               = 100\n",
        "max_steps                   = per_device_train_batch_size * logging_steps\n",
        "max_trainingset_size        = max_steps * per_device_train_batch_size # TODO freiem Grafikkarten Speicher ableiten\n",
        "max_steps                   = max_trainingset_size // per_device_train_batch_size # Schritte pro Epoche\n",
        "warmup_steps                = max_steps // 4\n",
        "save_steps                  = max_steps // 2\n",
        "eval_steps                  = save_steps\n",
        "max_sample_size             = 1200   # None -> no restriction\n",
        "max_trainingset_size        = per_device_train_batch_size * max_steps   # None -> no restriction\n",
        "fixed_training_set_size     = 100 * per_device_eval_batch_size          # None -> 20% der Originaldatensatz Größe \n",
        "num_train_epochs            = 1  # Anzahl Durchläufe für ein Dataset\n",
        "max_rounds                  = 1  # Anzahl Trainingsrunden über alle used_datasets\n",
        "\n",
        "learning_rate = per_device_train_batch_size * ((3e-4)/16)  # 3e-4/4 "
      ],
      "id": "worthy-resolution",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "motivated-budapest"
      },
      "source": [
        "## 1. Setup Directories"
      ],
      "id": "motivated-budapest"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stuffed-silicon",
        "outputId": "86132d31-d5e5-4323-8116-b9606278ad40"
      },
      "source": [
        "local_nlp_base_dir = f'{home}/shared'\n",
        "local_data_base_dir = f'{local_nlp_base_dir}/NLP-Data'\n",
        "local_data_dir = f'{local_data_base_dir}/audio'\n",
        "extern_nlp_base_dir = None\n",
        "extern_data_base_dir = None\n",
        "\n",
        "runs_on_colab = (home == '/root')\n",
        "print( f'runs on colab: {runs_on_colab}')\n",
        "\n",
        "if not runs_on_colab:\n",
        "    os.environ['http_proxy'] = 'http://192.168.8.50:3128'\n",
        "    os.environ['https_proxy'] = 'http://192.168.8.50:3128'\n",
        "else:\n",
        "    # to get access to the datasets we use gdrive\n",
        "    use_gdrive = True\n",
        "    # install packages\n",
        "    !pip install datasets==1.4.1\n",
        "    !pip install transformers==4.4.0\n",
        "    !pip install jiwer\n",
        "    !pip install torchaudio\n",
        "    !pip install librosa\n",
        "    # create local directories\n",
        "    !mkdir $local_nlp_base_dir\n",
        "    !mkdir $local_data_base_dir\n",
        "\n",
        "if use_gdrive:\n",
        "    gdrive_base = '/content/gdrive'    \n",
        "    extern_nlp_base_dir = f'{gdrive_base}/MyDrive'\n",
        "    extern_data_base_dir = f'{extern_nlp_base_dir}/NLP-Data'\n",
        "    extern_data_dir = f'{extern_data_base_dir}/audio'\n",
        "\n",
        "    if not os.path.isdir(gdrive_base):\n",
        "        from google.colab import drive\n",
        "        drive.mount(gdrive_base)\n",
        "\n",
        "if not os.path.isdir(local_data_dir):\n",
        "    !mkdir $local_data_dir\n",
        "\n",
        "if extern_nlp_base_dir:\n",
        "    model_dir = f'{extern_nlp_base_dir}/NLP-Models/GermanWave2Vec'\n",
        "else:\n",
        "    model_dir = f'{local_nlp_base_dir}/NLP-Models/GermanWave2Vec'\n",
        "\n",
        "# Use the Model from this Directory (Base: .../NLP-Models/GermanWave2Vec/)\n",
        "# None -> Start from 'facebook/wav2vec2-large-xlsr-53-german'\n",
        "trained_model_path = f'{model_dir}/{trained_model_directory}'\n",
        "git_views_dir = f'{local_nlp_base_dir}/gitviews'\n",
        "\n",
        "if not os.path.isdir(git_views_dir):\n",
        "    !mkdir $git_views_dir\n",
        "    !cd $git_views_dir; git clone https://github.com/ElUnrast/GermanWave2Vec.git"
      ],
      "id": "stuffed-silicon",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "runs on colab: True\n",
            "Requirement already satisfied: datasets==1.4.1 in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (2021.4.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (0.3.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (4.0.1)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (3.0.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (2.0.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.2 in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (0.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.4.1) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.4.1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets==1.4.1) (3.7.4.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.4.1) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==1.4.1) (2.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.2->datasets==1.4.1) (3.0.12)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.4.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.4.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.4.1) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.4.1) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.4.1) (1.15.0)\n",
            "Requirement already satisfied: transformers==4.4.0 in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (0.0.45)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (0.10.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.4.0) (1.19.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.4.0) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.4.0) (2020.12.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.4.0) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.4.0) (1.0.1)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.7/dist-packages (2.2.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.7/dist-packages (from jiwer) (0.12.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from jiwer) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from python-Levenshtein->jiwer) (56.1.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchaudio) (1.19.5)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.3.0)\n",
            "Requirement already satisfied: soundfile>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (56.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (20.9)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.9.0->librosa) (1.14.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pooch>=1.0->librosa) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2020.12.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.9.0->librosa) (2.20)\n",
            "mkdir: cannot create directory ‘/root/shared’: File exists\n",
            "mkdir: cannot create directory ‘/root/shared/NLP-Data’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wLJpgjxOFRM",
        "outputId": "b2464abb-2082-454c-b051-3ef53d1b363b"
      },
      "source": [
        "if runs_on_colab:\n",
        "    git_view_path = f'{git_views_dir}/GermanWave2Vec'\n",
        "    !cd $git_view_path; git fetch --all; git reset --hard origin/main\n",
        "\n",
        "script_path = f'{git_views_dir}/GermanWave2Vec/python'\n",
        "# Die Klassen aus GitHub sin in diesem Notebook inline definiert\n",
        "# sys.path.insert(0, script_path)"
      ],
      "id": "4wLJpgjxOFRM",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching origin\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), done.\n",
            "From https://github.com/ElUnrast/GermanWave2Vec\n",
            "   218586c..9f3ddfc  main       -> origin/main\n",
            "HEAD is now at 9f3ddfc Erstellt mit Colaboratory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "better-footwear"
      },
      "source": [
        "## 2. Check Runtime Properties"
      ],
      "id": "better-footwear"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "accompanied-wright",
        "outputId": "75fdc43b-c243-4f02-faca-cd371f94f62a"
      },
      "source": [
        "if 'cuda' == device:\n",
        "    gpu_info = !nvidia-smi\n",
        "    gpu_info = '\\n'.join(gpu_info)\n",
        "\n",
        "    if runs_on_colab:\n",
        "        if wait_for_v100 and not gpu_info.find('V100') >= 0:\n",
        "            print('The current GPU is not a V100')\n",
        "            print('Since you want to wait for a V100 the current session is aborted')\n",
        "            raise ValueError\n",
        "\n",
        "        if gpu_info.find('failed') >= 0:\n",
        "            print('For training, please use a VM with GPU!')\n",
        "            raise ValueError\n",
        "            \n",
        "        from psutil import virtual_memory\n",
        "        ram_gb = virtual_memory().total / 1e9\n",
        "        print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "        if ram_gb < 20:\n",
        "            print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "            print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "            print('re-execute this cell.')\n",
        "        else:\n",
        "            print('You are using a high-RAM runtime!')\n",
        "            \n",
        "    print(gpu_info)"
      ],
      "id": "accompanied-wright",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n",
            "Fri May 14 15:22:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "connected-liver"
      },
      "source": [
        "## 3 Install packages and do Imports"
      ],
      "id": "connected-liver"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "copyrighted-peeing"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
        "from transformers.trainer_pt_utils import LengthGroupedSampler, DistributedLengthGroupedSampler\n",
        "\n",
        "import json\n",
        "import collections\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import jiwer\n",
        "from jiwer import wer\n",
        "from datasets import load_metric\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# from SnippetDatasets import SnippetDatasets\n",
        "# from GermanSpeechToTextTranslater import GermanSpeechToTextTranslater"
      ],
      "id": "copyrighted-peeing",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6XbORYANDFf"
      },
      "source": [
        "# Kopie aus github 'SnippetDatasets.py'\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "from jiwer import wer\n",
        "from zipfile import ZipFile\n",
        "\n",
        "\n",
        "# Known Snippet Directories with content.csv, content-with_original.csv\n",
        "class SnippetDatasets:\n",
        "    def __init__(self, run_on_colab, local_audio_base_dir, git_repository=None, extern_audio_base_dir=None):\n",
        "        self.local_datasets = {}\n",
        "        self.extern_datasets = {}\n",
        "        self.used_datasets = []\n",
        "        self.local_audio_base_dir = local_audio_base_dir\n",
        "        self.extern_audio_base_dir = extern_audio_base_dir\n",
        "        self.git_repository = git_repository\n",
        "\n",
        "        for dir in self.directories_with_content(local_audio_base_dir):\n",
        "            self.local_datasets[os.path.basename(dir)] = dir\n",
        "\n",
        "        if extern_audio_base_dir:\n",
        "            for zip_file in glob.glob(f'{extern_audio_base_dir}/*.zip'):\n",
        "                ds_id = os.path.basename(zip_file)[0:-4]\n",
        "                self.extern_datasets[ds_id] = zip_file\n",
        "\n",
        "    def directories_with_content(self, dir):\n",
        "        result = []\n",
        "\n",
        "        if self.has_content(dir) or self.has_content_original(dir):\n",
        "            result.append(dir)\n",
        "        else:\n",
        "            sub_dir = [f'{dir}/{d}' for d in os.listdir(dir) if os.path.isdir(f'{dir}/{d}')]\n",
        "\n",
        "            for d in sub_dir:\n",
        "                result.extend(self.directories_with_content(d))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def use_datasets(self, datasets_to_use):\n",
        "        if isinstance(datasets_to_use, list):\n",
        "            for ds in datasets_to_use:\n",
        "                self.use_dataset(ds)\n",
        "        else:\n",
        "            self.use_dataset(datasets_to_use)\n",
        "\n",
        "    def use_dataset(self, ds_id_to_use):\n",
        "        if ds_id_to_use in self.local_datasets:\n",
        "            self.used_datasets.append(ds_id_to_use)\n",
        "        elif ds_id_to_use in self.extern_datasets:\n",
        "            # TODO: Falls nicht genug Speicher da ist sollten nicht verwendete externe Datasets,\n",
        "            #       welche lokal vorhanden sind, lokal gelöscht werden.\n",
        "            zip_file_name = f'{self.extern_audio_base_dir}/{ds_id_to_use}.zip'\n",
        "            print(f'Download and extract {zip_file_name} from gdrive')\n",
        "\n",
        "            with ZipFile(zip_file_name, 'r') as zip:\n",
        "                zip.extractall(self.local_audio_base_dir)\n",
        "\n",
        "            self.local_datasets[ds_id_to_use] = f'{self.local_audio_base_dir}/{ds_id_to_use}'\n",
        "            self.used_datasets.append(ds_id_to_use)\n",
        "\n",
        "    def has_content(self, id_or_directory):\n",
        "        return os.path.isfile(f'{self._get_directory(id_or_directory)}/content.csv')\n",
        "\n",
        "    def has_content_original(self, id_or_directory):\n",
        "        return os.path.isfile(f'{self._get_directory(id_or_directory)}/content-with_original.csv')\n",
        "\n",
        "    def has_translation(self, id_or_directory):\n",
        "        return os.path.isfile(f'{self._get_directory(id_or_directory)}/content-translated.csv')\n",
        "\n",
        "    def has_translation_with_original(self, id_or_directory):\n",
        "        return os.path.isfile(f'{self._get_directory(id_or_directory)}/content-translated-with_original.csv')\n",
        "\n",
        "    def needs_translation(self, directory):\n",
        "        if self.has_content(directory):\n",
        "            return not self.has_translation(directory)\n",
        "        else:\n",
        "            return not self.has_translation_with_original(directory)\n",
        "\n",
        "    def get_snippet_directory(self, ds_id):\n",
        "        return self.local_datasets[ds_id]\n",
        "\n",
        "    def get_ds_git_directory(self, ds_id):\n",
        "        if self.git_repository:\n",
        "            snippet_directory = get_snippet_directory(ds_id)\n",
        "            \n",
        "            if snippet_directory:\n",
        "                return f'{self.git_repository}/datasets/{snippet_directory[len(self.local_audio_base_dir):]}'\n",
        "        \n",
        "        return None;\n",
        "\n",
        "    def load_ds_content(self, id_or_directory):\n",
        "        # Action: translate -> find original\n",
        "        return self._get_dataframe(id_or_directory, 'content.csv')\n",
        "\n",
        "    def load_ds_content_with_original(self, id_or_directory):\n",
        "        # Action: translate -> train\n",
        "        return self._get_dataframe(id_or_directory, 'content-with_original.csv')\n",
        "\n",
        "    def load_ds_content_translated(self, id_or_directory):\n",
        "        # File for Action: find original\n",
        "        return self._get_dataframe(id_or_directory, 'content-translated.csv')\n",
        "\n",
        "    def load_ds_content_translated_with_original(self, id_or_directory):\n",
        "        # File for Action: train or repeated translation\n",
        "        return self._get_dataframe(id_or_directory, 'content-translated-with_original.csv')\n",
        "    \n",
        "    def _get_directory(self, id_or_directory):\n",
        "        if id_or_directory in self.local_datasets:\n",
        "            return self.get_snippet_directory(id_or_directory)\n",
        "        \n",
        "        return id_or_directory\n",
        "    \n",
        "    def _get_dataframe(self, id_or_directory, file_name):\n",
        "        print(f'Loading Dataset: {id_or_directory} - {file_name}')\n",
        "        self.use_dataset(id_or_directory)\n",
        "        ds_directory = self._get_directory(id_or_directory)\n",
        "        pandas_df = pd.read_csv(f'{ds_directory}/{file_name}', sep=';')\n",
        "        truncated_ds = pandas_df\n",
        "        \n",
        "        if 'OriginalText' in pandas_df.columns:\n",
        "            print(f'Pruning Dataset {id_or_directory} with {pandas_df.shape[0]} Entries')\n",
        "\n",
        "            if 'Length' in pandas_df.columns:\n",
        "                truncated_ds = truncated_ds[(truncated_ds.Length <= 4000) & (truncated_ds.Length >= 31)]\n",
        "                print(f' - {truncated_ds.shape[0]} Entries left after Length Cut (min=31, max=4000)')\n",
        "\n",
        "            if 'Action' in pandas_df.columns:\n",
        "                truncated_ds = truncated_ds[~truncated_ds.Action.str.startswith('exclude')]\n",
        "                print(f' - {truncated_ds.shape[0]} Entries left after Action Cut')\n",
        "        else:\n",
        "            if pandas_df.Length.max() > (1600 * 3):\n",
        "                raise ValueError\n",
        "            \n",
        "        if pandas_df.shape[0] != truncated_ds.shape[0]:\n",
        "            print(f'Dataset was truncated from {pandas_df.shape[0]} to {truncated_ds.shape[0]} Entries. Saving Backup.')\n",
        "            pandas_df.to_csv(f'{ds_directory}/original-{file_name}', sep=';', index=False)\n",
        "            truncated_ds.to_csv(f'{ds_directory}/{file_name}', sep=';', index=False)\n",
        "        \n",
        "        pandas_df = truncated_ds\n",
        "        return pandas_df\n",
        "\n",
        "\n",
        "def calc_wer(ds_with_translation_and_original, chunk_size=1000):\n",
        "    if 'Translated1' in ds_with_translation_and_original.columns:\n",
        "        translation_column = ds_with_translation_and_original.Translated1\n",
        "    else:\n",
        "        translation_column = ds_with_translation_and_original.Translated0\n",
        "\n",
        "    return chunked_wer(\n",
        "        targets=ds_with_translation_and_original.OriginalText.tolist(),\n",
        "        predictions=translation_column.tolist(),\n",
        "        chunk_size=chunk_size\n",
        "    )\n",
        "\n",
        "# Chunked version, see https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586/5:\n",
        "def chunked_wer(targets, predictions, chunk_size=1000):\n",
        "    if chunk_size is None:\n",
        "        return jiwer.wer(targets, predictions)\n",
        "\n",
        "    start = 0\n",
        "    end = chunk_size\n",
        "    H, S, D, I = 0, 0, 0, 0\n",
        "\n",
        "    while start < len(targets):\n",
        "        chunk_metrics = jiwer.compute_measures(targets[start:end], predictions[start:end])\n",
        "        H = H + chunk_metrics[\"hits\"]\n",
        "        S = S + chunk_metrics[\"substitutions\"]\n",
        "        D = D + chunk_metrics[\"deletions\"]\n",
        "        I = I + chunk_metrics[\"insertions\"]\n",
        "        start += chunk_size\n",
        "        end += chunk_size\n",
        "\n",
        "    return float(S + D + I) / float(H + S + D)\n"
      ],
      "id": "e6XbORYANDFf",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHRy8uUpNcZ6"
      },
      "source": [
        "# Kopie aus github 'GermanSpeechToTextTranslater.py'\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Union\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.cuda.amp import autocast\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
        "from transformers.trainer_pt_utils import LengthGroupedSampler, DistributedLengthGroupedSampler\n",
        "\n",
        "import json\n",
        "import collections\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import jiwer\n",
        "from jiwer import wer\n",
        "from datasets import load_metric\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "class GermanSpeechToTextTranslater:\n",
        "    def __init__(\n",
        "            self,\n",
        "            model=None,\n",
        "            processor=None,\n",
        "            # language_tool=None,\n",
        "            # resampled_dir=None,\n",
        "            model_name=None,\n",
        "            default_model_name='facebook/wav2vec2-large-xlsr-53-german',\n",
        "            device='cuda'\n",
        "    ):\n",
        "        self.device = device\n",
        "        self.model_name = model_name if model_name else default_model_name\n",
        "        print(f'Using Model: {self.model_name}')\n",
        "        print('Loading processor')\n",
        "        # Anlegen eines eigenen Processors, da in Wav2Vec2Processor.from_pretrained(facebook/wav2vec2-large-xlsr-53-german)\n",
        "        # kein ß enthalten ist\n",
        "        self.my_processor = processor if processor else self.create_processor()\n",
        "        print('Loading metric')\n",
        "        self.my_metric = load_metric('wer')\n",
        "        self.trained_epochs = 0\n",
        "        \n",
        "        # if os.path.isfile(f'{self.model_name}/pytorch_model.bin'):\n",
        "        #     if os.path.isfile(f'{self.model_name}/trained_epochs.json'):\n",
        "        #         with open(f'{self.model_name}/trained_epochs.json', 'r') as json_file:\n",
        "        #             self.trained_epochs = json.load(json_file)['trained_epochs']\n",
        "        #     else:\n",
        "        #         with open(f'{self.model_name}/trained_epochs.json', 'w') as json_file:\n",
        "        #             json.dump({'trained_epochs' : self.trained_epochs}, json_file)\n",
        "        \n",
        "        # TODO: in abgeleitete Klasse verlagern\n",
        "        # print('Loading language tool')\n",
        "        # self.my_tool = language_tool if language_tool else language_tool_python.LanguageTool('de-DE')\n",
        "        print('Loading model')\n",
        "\n",
        "        if model:\n",
        "            self.my_model = model\n",
        "        else:\n",
        "            if model_name:\n",
        "                self.my_model = Wav2Vec2ForCTC.from_pretrained(self.model_name).to(device)\n",
        "            else:\n",
        "                self.my_model = Wav2Vec2ForCTC.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    # activation_dropout=0.055\n",
        "                    attention_dropout=0.1,  # 0.094\n",
        "                    hidden_dropout=0.1,  #\n",
        "                    feat_proj_dropout=0.0,  # 0.04\n",
        "                    mask_time_prob=0.05,  # 0.08\n",
        "                    layerdrop=0.1,  # 0.04\n",
        "                    gradient_checkpointing=True,\n",
        "                    ctc_loss_reduction=\"mean\",\n",
        "                    pad_token_id=self.my_processor.tokenizer.pad_token_id,\n",
        "                    vocab_size=len(self.my_processor.tokenizer)\n",
        "                ).to(device)\n",
        "        self.my_model.freeze_feature_extractor()\n",
        "\n",
        "    def reload_from_checkpoint(self, checkpoint):\n",
        "        del self.my_model\n",
        "        torch.cuda.empty_cache()\n",
        "        self.model_name = checkpoint\n",
        "        print(f'Using Model: {self.model_name}')\n",
        "        self.my_model = Wav2Vec2ForCTC.from_pretrained(self.model_name).to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "        self.my_model.freeze_feature_extractor()\n",
        "\n",
        "    # def translate(self, audio_file_name):\n",
        "    #    translation = self.translate_audio(audio_file_name)\n",
        "    #    return self.my_tool.correct(translation), translation\n",
        "\n",
        "    def load_as_sr16000(self, audio_file_name):\n",
        "        new_path = None\n",
        "\n",
        "        if not audio_file_name.endswith('.mp3'):\n",
        "            samples, sampling_rate = librosa.load(audio_file_name, sr=16_000)  # Downsample to 16kHz\n",
        "        else:\n",
        "            # print( f'load {audio_file_name}')\n",
        "            samples, sampling_rate = torchaudio.load(audio_file_name)\n",
        "            # print(f'samples.shape : {samples.shape}, sampling_rate : {sampling_rate}')\n",
        "            samples = samples[0]\n",
        "\n",
        "            if sampling_rate != 16_000:\n",
        "                # print( f'Converting from {sampling_rate}')\n",
        "                samples = librosa.resample(np.asarray(samples), sampling_rate, 16_000)\n",
        "                # funktioniert auch:\n",
        "                # samples = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16_000)(samples).squeeze().numpy()\n",
        "\n",
        "        if samples.shape[0] == 2:\n",
        "            samples = samples[0]\n",
        "\n",
        "        if isinstance(samples, (np.ndarray, np.generic)):\n",
        "            samples = torch.from_numpy(samples).float().flatten()\n",
        "        else:\n",
        "            samples = samples.squeeze().numpy()\n",
        "\n",
        "        if new_path != None:\n",
        "            torch.save(samples, new_path)\n",
        "\n",
        "        samples_size = samples.shape[0]\n",
        "        return samples, samples.shape[0]\n",
        "\n",
        "    def audio_to_cuda_inputs(self, audio_file_name):\n",
        "        samples, samples_size = self.load_as_sr16000(audio_file_name)\n",
        "        return self.my_processor(samples, return_tensors=\"pt\", sampling_rate=16_000).input_values, samples_size\n",
        "\n",
        "    def translate_audio(self, audio_file_name):\n",
        "        samples, samples_size = self.audio_to_cuda_inputs(audio_file_name)\n",
        "        samples_size = samples_size\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = self.my_model(samples.to(self.device)).logits\n",
        "\n",
        "        # Storing predicted ids\n",
        "        predicted_ids = torch.argmax(logits, dim=-1)\n",
        "        # Converting audio to text - Passing the prediction to the tokenzer decode to get the transcription\n",
        "        return self.my_processor.decode(predicted_ids[0]), samples_size\n",
        "\n",
        "    def translate_and_extend_dataset_from_directory(self, ds_loader, id_or_directory):\n",
        "        if not ds_loader.needs_translation(id_or_directory):\n",
        "            return\n",
        "\n",
        "        print(f'Translating and extend Dataset: {id_or_directory}')\n",
        "        has_original = ds_loader.has_content_original(id_or_directory)\n",
        "\n",
        "        if not has_original:\n",
        "            ds = ds_loader.load_ds_content(id_or_directory)\n",
        "        else:\n",
        "            ds = ds_loader.load_ds_content_with_original(id_or_directory)\n",
        "\n",
        "        ds_dir_name = ds_loader.get_snippet_directory(id_or_directory)\n",
        "        translated_list, size_list = self.translate_dataset(ds_dir_name, ds)\n",
        "\n",
        "        if 'Translated1' in ds.columns:\n",
        "            ds['Size'] = size_list\n",
        "            ds['Translated0'] = translated_list\n",
        "            del ds['Translated1']\n",
        "        elif 'Translated0' in ds.columns:\n",
        "            ds['Size'] = size_list\n",
        "            ds['Translated0'] = translated_list\n",
        "        else:\n",
        "            ds.insert(loc=8, column='Size', value=size_list)\n",
        "            ds.insert(loc=9, column='Translated0', value=translated_list)\n",
        "\n",
        "        if not has_original:\n",
        "            ds.to_csv(f'{ds_dir_name}/content-translated.csv', sep=';', index=False)\n",
        "        else:\n",
        "            ds.to_csv(f'{ds_dir_name}/content-translated-with_original.csv', sep=';', index=False)\n",
        "\n",
        "    def translate_dataset(self, mp3_dir, ds):\n",
        "        if isinstance(ds, GermanTrainingWav2Vec2Dataset):\n",
        "            files = [f'{mp3_dir}/{file_name}' for file_name in ds.paths]\n",
        "        else:\n",
        "            files = [f'{mp3_dir}/{file_name}' for file_name in ds.Datei]\n",
        "\n",
        "        return self.translate_audio_files(files)\n",
        "\n",
        "    def translate_audio_files(self, files):\n",
        "        translated_list = []\n",
        "        size_list = []\n",
        "        idx = 0\n",
        "\n",
        "        for file_name in tqdm_notebook(files):\n",
        "            if (idx % 20) == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            idx = idx + 1\n",
        "            translated, sample_size = self.translate_audio(file_name)\n",
        "            translated_list.append(translated)\n",
        "            size_list.append(sample_size)\n",
        "\n",
        "        return translated_list, size_list\n",
        "\n",
        "    def split_dataset(\n",
        "            self,\n",
        "            pandas_df,\n",
        "            max_trainingset_size=25000,\n",
        "            max_sample_size=1000,\n",
        "            use_only_incorrect_translated=True\n",
        "    ):\n",
        "        used_df = pandas_df\n",
        "        has_action = 'Action' in pandas_df.columns\n",
        "\n",
        "        if has_action:\n",
        "            used_df = used_df[(used_df.Action == 'train') | (used_df.Action == 'translate')]\n",
        "            print(f' - {used_df.shape[0]} Entries left after Action Cut')\n",
        "\n",
        "        used_df = used_df[used_df.Length <= max_sample_size] if max_sample_size else pandas_df\n",
        "        print(f' - {used_df.shape[0]} Entries left after Length Cut (max={max_sample_size})')\n",
        "        used_df = used_df[used_df.Length > 30]\n",
        "        print(f' - {used_df.shape[0]} Entries left after minimal Size Cut (min=31)')\n",
        "\n",
        "        # Test Dataset should be fixed_training_set_size Entries long\n",
        "        test_percentage = fixed_training_set_size / used_df.shape[0] if fixed_training_set_size else 0.2\n",
        "\n",
        "        train, test = train_test_split(used_df, test_size=test_percentage, random_state=143)\n",
        "        print(f'Training Dataset Size: {train.shape[0]}, Validation Dataset Size {test.shape[0]}')\n",
        "\n",
        "        if use_only_incorrect_translated:\n",
        "            if 'Translated1' in train.columns:\n",
        "                train = train[(train.OriginalText != train.Translated1)]\n",
        "            else:\n",
        "                train = train[(train.OriginalText != train.Translated0)]\n",
        "\n",
        "            print(f' - {train.shape[0]} Entries left after first Recognition Cut')\n",
        "\n",
        "        train = sklearn.utils.shuffle(train)\n",
        "\n",
        "        if max_trainingset_size:\n",
        "            train = train[:min(train.shape[0], max_trainingset_size)]\n",
        "            print(f' - {train.shape[0]} left after Entries Max Samples Cut (max={max_trainingset_size})')\n",
        "\n",
        "        print(f'Training Dataset Size: {train.shape[0]}, Validation Dataset Size {test.shape[0]}')\n",
        "        return train, test\n",
        "\n",
        "    def get_trainer(\n",
        "            self,\n",
        "            training_args,\n",
        "            snippet_directory,\n",
        "            train_ds,\n",
        "            test_ds,\n",
        "            use_grouped_legth_trainer=False\n",
        "    ):\n",
        "        train_dataset = GermanTrainingWav2Vec2Dataset(self, snippet_directory, train_ds, 'train')\n",
        "        test_dataset = GermanTrainingWav2Vec2Dataset(self, snippet_directory, test_ds, 'eval') if not test_ds.empty else None\n",
        "        data_collator = DataCollatorCTCWithPadding(processor=self.my_processor, padding=True)\n",
        "\n",
        "        if use_grouped_legth_trainer:\n",
        "            trainer = GroupedLengthsTrainer(\n",
        "                model=self.my_model,\n",
        "                data_collator=data_collator,\n",
        "                args=training_args,\n",
        "                compute_metrics=self.compute_metrics,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=test_dataset,\n",
        "                tokenizer=self.my_processor.feature_extractor,\n",
        "                train_seq_lengths=train_dataset.input_seq_lengths\n",
        "            )\n",
        "        else:\n",
        "            trainer = Trainer(\n",
        "                model=self.my_model,\n",
        "                data_collator=data_collator,\n",
        "                args=training_args,\n",
        "                compute_metrics=self.compute_metrics,\n",
        "                train_dataset=train_dataset,\n",
        "                eval_dataset=test_dataset,\n",
        "                tokenizer=self.my_processor.feature_extractor,\n",
        "            )\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def create_processor(self):\n",
        "        vocab_file_name = 'vocab.json'\n",
        "\n",
        "        if not os.path.isfile(vocab_file_name):\n",
        "            vocab_dict = {\n",
        "                '<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4,\n",
        "                'e': 5, 'n': 6, 'i': 7, 's': 8, 'r': 9, 't': 10, 'a': 11,\n",
        "                'h': 12, 'd': 13, 'u': 14, 'l': 15, 'c': 16, 'g': 17, 'm': 18,\n",
        "                'o': 19, 'b': 20, 'w': 21, 'f': 22, 'k': 23, 'z': 24, 'v': 25,\n",
        "                'ü': 26, 'p': 27, 'ä': 28, 'ö': 29, 'j': 30, 'y': 31, \"'\": 32,\n",
        "                'x': 33, 'q': 34, 'ß': 35\n",
        "            }\n",
        "\n",
        "            with open(vocab_file_name, 'w') as vocab_file:\n",
        "                json.dump(vocab_dict, vocab_file)\n",
        "\n",
        "        tokenizer = Wav2Vec2CTCTokenizer(\n",
        "            \"vocab.json\",\n",
        "            unk_token=\"<unk>\",\n",
        "            pad_token=\"<pad>\",\n",
        "            word_delimiter_token=\"|\"\n",
        "        )\n",
        "        feature_extractor = Wav2Vec2FeatureExtractor(\n",
        "            feature_size=1, sampling_rate=16_000, padding_value=0.0, do_normalize=True, return_attention_mask=True\n",
        "        )\n",
        "        return Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "\n",
        "    def test(self, ds_id, pandas_df, mp3_dir, diff_file_extension, diff_calc_wer=False):\n",
        "        if not 'Autor' in pandas_df.columns:\n",
        "            raise ValueError\n",
        "\n",
        "        wer_result = 1.0\n",
        "        translation_column_name = 'Translated1'\n",
        "\n",
        "        print(f'Translate all')\n",
        "        predictions, _ = self.translate_dataset(mp3_dir, pandas_df)\n",
        "        pandas_df[translation_column_name] = predictions\n",
        "        pandas_df.to_csv(f'{mp3_dir}/content-translated-with_original.csv', sep=';', index=False)\n",
        "\n",
        "        if diff_calc_wer:\n",
        "            print('Calculate WER')\n",
        "            wer_result = self.calc_wer(pandas_df)\n",
        "            print(f'WER: {wer_result}')\n",
        "\n",
        "        print('Saving diff files')\n",
        "        truncated_ds = pandas_df[\n",
        "            ((pandas_df.Action == 'train') | (pandas_df.Action == 'translate')) & (pandas_df.Length <= 1200)\n",
        "        ]\n",
        "        bad_translation_ds = truncated_ds[truncated_ds[translation_column_name] != truncated_ds['OriginalText']]\n",
        "\n",
        "        translations = bad_translation_ds[translation_column_name].tolist()\n",
        "        original_texts = bad_translation_ds['OriginalText'].tolist()\n",
        "        file_names = bad_translation_ds['Datei'].tolist()\n",
        "\n",
        "        orig_file = open(f'{model_dir}/{ds_id}-original-{diff_file_extension}.txt', 'w')\n",
        "        translated_file = open(f'{model_dir}/{ds_id}-translated-{diff_file_extension}.txt', 'w')\n",
        "\n",
        "        for file_name, original_text, translation in zip(file_names, original_texts, translations):\n",
        "            orig_file.write(f'{file_name}, {original_text}\\n')\n",
        "            translated_file.write(f'{file_name}, {translation}\\n')\n",
        "\n",
        "        orig_file.close()\n",
        "        translated_file.close()\n",
        "\n",
        "        return bad_translation_ds, truncated_ds, wer_result\n",
        "\n",
        "    def train(\n",
        "        dataset_loader, \n",
        "        ds_to_train, \n",
        "        max_trainingset_size, \n",
        "        max_rounds, \n",
        "        num_train_epochs,\n",
        "        num_steps_per_epoche, # max_steps,\n",
        "        per_device_train_batch_size,\n",
        "        per_device_eval_batch_size,\n",
        "        gradient_accumulation_steps,\n",
        "        logging_steps,\n",
        "        learning_rate,\n",
        "        warmup_steps,\n",
        "        early_stopping=0.2\n",
        "    ):\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=trained_model_path,\n",
        "            group_by_length=True,\n",
        "            per_device_train_batch_size=per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=per_device_eval_batch_size, \n",
        "            gradient_accumulation_steps=gradient_accumulation_steps, \n",
        "            evaluation_strategy=\"steps\",\n",
        "            max_steps=num_steps_per_epoche,         \n",
        "            fp16=True,\n",
        "            # save_steps=save_steps,     \n",
        "            # eval_steps=eval_steps,     \n",
        "            logging_steps=logging_steps, \n",
        "            learning_rate=learning_rate,\n",
        "            warmup_steps=warmup_steps,   \n",
        "            # save_total_limit=2\n",
        "        )\n",
        "\n",
        "        for runde in range(max_rounds):\n",
        "            print(f'Starting round {runde} of {max_rounds}')\n",
        "\n",
        "            for ds_id in ds_to_train:\n",
        "                if not dataset_loader.has_translation_with_original(ds_id):\n",
        "                    ds_to_train.remove(ds_id)\n",
        "                    continue\n",
        "\n",
        "                WANDB_NOTES=ds_id\n",
        "                pandas_df = dataset_loader.load_ds_content_translated_with_original(ds_id)\n",
        "                print(f'Dataset - {ds_id} loaded with {pandas_df.shape[0]} Entries')\n",
        "                mp3_dir = dataset_loader.get_snippet_directory(ds_id)\n",
        "              \n",
        "                for epoche in range(num_train_epochs):\n",
        "                    print(f'Starting round {runde} of {max_rounds}, epoche {epoche} of {num_train_epochs}')\n",
        "                    print(f'Splitting Dataset {ds_id} with {pandas_df.shape[0]} Entries')\n",
        "                    bad_translation_ds, truncated_ds, wer_result = translator.test(\n",
        "                        ds_id, \n",
        "                        pandas_df, \n",
        "                        mp3_dir, \n",
        "                        diff_file_extension=f'{epoche}-{runde}', \n",
        "                        diff_calc_wer=True\n",
        "                    )\n",
        "                  \n",
        "                    if (bad_translation_ds.shape[0] > (truncated_ds.shape[0] * early_stopping)) and (wer_result < early_stopping):                    \n",
        "                        if max_trainingset_size:\n",
        "                            train = bad_translation_ds[:min(bad_translation_ds.shape[0], max_trainingset_size)]\n",
        "                            print(f' - {train.shape[0]} left after Entries Max Samples Cut (max={max_trainingset_size})')\n",
        "                      \n",
        "                        print(f'Creating Trainer for {ds_id}')\n",
        "                        trainer = translator.get_trainer(\n",
        "                            training_args, \n",
        "                            mp3_dir,             \n",
        "                            train_pandas_ds,\n",
        "                            None,  # test_pandas_ds,\n",
        "                            use_grouped_legth_trainer=False\n",
        "                        )\n",
        "                        print(f'Training of Dataset: {ds_id}')\n",
        "                        train_result = trainer.train()\n",
        "\n",
        "                        print(f'Save Model')\n",
        "                        trainer.save_model()\n",
        "                        metrics = train_result.metrics\n",
        "                        max_train_samples = train_pandas_ds.shape[0]\n",
        "                        metrics[\"train_samples\"] = min(max_train_samples, train_pandas_ds.shape[0])\n",
        "                        trainer.log_metrics(\"train\", metrics)\n",
        "                        trainer.save_metrics(\"train\", metrics)\n",
        "                        trainer.save_state()\n",
        "                        torch.cuda.empty_cache()\n",
        "                        self.trained_epochs = self.trained_epochs + 1\n",
        "                        with open(f'{self.model_name}/trained_epochs.json', 'w') as json_file:\n",
        "                            json.dump({'trained_epochs', self.trained_epochs}, json_file)\n",
        "                    else:\n",
        "                        # mindestens 98% der Sätze wurde korrekt übersetzt. Überprüfung der Problemfälle ist angebracht.\n",
        "                        # Es hat sich gezeigt, dass das Ergebnis wieder schlechter werden kann.\n",
        "                        print('Early stopping!')\n",
        "                        ds_to_train.remove(ds_id)\n",
        "                        break\n",
        "\n",
        "                print(f'final check und update of {ds_id}')\n",
        "                bad_translation_ds, truncated_ds, wer_result = translator.test(\n",
        "                    ds_id, \n",
        "                    pandas_df, \n",
        "                    mp3_dir, \n",
        "                    diff_file_extension=f'{epoche}-{runde}', \n",
        "                    diff_calc_wer=True\n",
        "                )\n",
        "\n",
        "        print('Training finisched!')\n",
        "    \n",
        "    def compute_metrics(self, pred):\n",
        "        # we do not want to group tokens when computing the metrics\n",
        "        label_str = self.my_processor.batch_decode(pred.label_ids, group_tokens=False)\n",
        "        pred_logits = pred.predictions\n",
        "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "        pred.label_ids[pred.label_ids == -100] = self.my_processor.tokenizer.pad_token_id\n",
        "        pred_str = self.my_processor.batch_decode(pred_ids)\n",
        "\n",
        "        # Bugs: strings in the list of strings 'pred_str' have 2 extra characters ' ' and first character in vocab_dict\n",
        "        #       strings in the list of strings 'label_str' have extra charecters '[UNK]'\n",
        "        # Correction:\n",
        "        # pred_str = [item[:-2] for item in pred_str]\n",
        "\n",
        "        def g(s):\n",
        "            s = s.strip()\n",
        "\n",
        "            while s[-5:] == \"<unk>\":\n",
        "                s = s[:-5]\n",
        "\n",
        "            return s\n",
        "\n",
        "        label_str_c = [g(item) for item in label_str]\n",
        "\n",
        "        # def f(s):\n",
        "        #     if len(s) <= 2:    # maybe 2 -> 1 later\n",
        "        #         return s\n",
        "        #     else:\n",
        "        #         return s[:-2]   # maybe 2 -> 1 later\n",
        "\n",
        "        # pred_str_c = [f(item) for item in pred_str]\n",
        "        pred_str_c = pred_str\n",
        "        # print(f'\"{label_str_c}\" - \"{pred_str_c}\"')\n",
        "        return {\"wer\": jiwer.compute_measures(label_str_c, pred_str_c)[\"wer\"]}\n",
        "\n",
        "\n",
        "class GermanTrainingWav2Vec2Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, german_speech_translator, snippet_directory, ds, split):\n",
        "        super().__init__()\n",
        "        assert split in {'train', 'eval'}\n",
        "        self.split = split\n",
        "        self.snippet_directory = snippet_directory\n",
        "        self.german_speech_translator = german_speech_translator\n",
        "        self.max_input_length_quantile = .98\n",
        "        self.max_input_length = None\n",
        "\n",
        "        if split == 'train':\n",
        "            self.input_seq_lengths = ds['Size'].tolist()\n",
        "            self.max_input_length = torch.tensor(self.input_seq_lengths).float() \\\n",
        "                .quantile(self.max_input_length_quantile).int().item()\n",
        "\n",
        "        self.labels = ds['OriginalText'].tolist()\n",
        "        self.paths = ds['Datei'].tolist()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mp3_file = f'{self.snippet_directory}/{self.paths[idx]}'\n",
        "        # print(f'Loading MP3: {mp3_file}')\n",
        "        inputs, _ = self.german_speech_translator.audio_to_cuda_inputs(\n",
        "            mp3_file\n",
        "        )\n",
        "\n",
        "        inputs = inputs.squeeze()\n",
        "        # print( f'Cuda Inputs created. {type(inputs)}, {inputs.shape}')\n",
        "        if self.split == 'train':\n",
        "            inputs = inputs[:self.max_input_length]\n",
        "\n",
        "        label_str = self.labels[idx]\n",
        "        # print(f'Label: {type(label_str)}')\n",
        "\n",
        "        processor = self.german_speech_translator.my_processor\n",
        "\n",
        "        with processor.as_target_processor():\n",
        "            label = processor(label_str).input_ids\n",
        "\n",
        "        return {'input_values': inputs, 'labels': label}\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "    \"\"\"\n",
        "    Data collator that will dynamically pad the inputs received.\n",
        "    Args:\n",
        "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
        "            The processor used for proccessing the data.\n",
        "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
        "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
        "            among:\n",
        "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
        "              sequence if provided).\n",
        "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
        "              maximum acceptable input length for the model if that argument is not provided.\n",
        "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
        "              different lengths).\n",
        "        max_length (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
        "        max_length_labels (:obj:`int`, `optional`):\n",
        "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
        "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
        "            If set will pad the sequence to a multiple of the provided value.\n",
        "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
        "            7.5 (Volta).\n",
        "    \"\"\"\n",
        "    processor: Wav2Vec2Processor\n",
        "    padding: Union[bool, str] = True\n",
        "    max_length: Optional[int] = None\n",
        "    max_length_labels: Optional[int] = None\n",
        "    pad_to_multiple_of: Optional[int] = None\n",
        "    pad_to_multiple_of_labels: Optional[int] = None\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        # print(f'Padding input: {self.padding}')\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            max_length=self.max_length,\n",
        "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        with self.processor.as_target_processor():\n",
        "            labels_batch = self.processor.pad(\n",
        "                label_features,\n",
        "                padding=self.padding,\n",
        "                max_length=self.max_length_labels,\n",
        "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "# solution from https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586/6\n",
        "class GroupedLengthsTrainer(Trainer):\n",
        "    # length_field_name should possibly be part of TrainingArguments instead\n",
        "    def __init__(self, train_seq_lengths: List[int], *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.train_seq_lengths = train_seq_lengths\n",
        "\n",
        "    def _get_train_sampler(self) -> Optional[torch.utils.data.sampler.Sampler]:\n",
        "        if isinstance(self.train_dataset, torch.utils.data.IterableDataset) or not isinstance(\n",
        "                self.train_dataset, collections.abc.Sized\n",
        "        ):\n",
        "            return None\n",
        "\n",
        "        # Build the sampler.\n",
        "        if self.args.group_by_length:\n",
        "            # lengths = self.train_dataset[self.length_field_name] if self.length_field_name is not None else None\n",
        "            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n",
        "\n",
        "            if self.args.world_size <= 1:\n",
        "                print('Using LengthGroupedSampler')\n",
        "                return LengthGroupedSampler(\n",
        "                    self.train_dataset, self.args.train_batch_size, lengths=self.train_seq_lengths,\n",
        "                    model_input_name=model_input_name\n",
        "                )\n",
        "            else:\n",
        "                print('Using DistributedLengthGroupedSampler')\n",
        "                return DistributedLengthGroupedSampler(\n",
        "                    self.train_dataset,\n",
        "                    self.args.train_batch_size,\n",
        "                    num_replicas=self.args.world_size,\n",
        "                    rank=self.args.process_index,\n",
        "                    lengths=self.train_seq_lengths,\n",
        "                    model_input_name=model_input_name,\n",
        "                )\n",
        "\n",
        "        else:\n",
        "            return super()._get_train_sampler()"
      ],
      "id": "VHRy8uUpNcZ6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "certified-singing"
      },
      "source": [
        "## 4 Initialize Helper Classes"
      ],
      "id": "certified-singing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unexpected-ideal",
        "scrolled": true
      },
      "source": [
        "my_datasets = SnippetDatasets(\n",
        "    runs_on_colab, \n",
        "    local_audio_base_dir=local_data_dir, \n",
        "    extern_audio_base_dir=extern_data_dir\n",
        ")"
      ],
      "id": "unexpected-ideal",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "measured-craps",
        "scrolled": true,
        "outputId": "eb73d35b-62ab-4619-9af7-b30aacabcb18"
      },
      "source": [
        "model_path = None\n",
        "\n",
        "if checkpoint:\n",
        "    model_path = f'{trained_model_path}/{checkpoint}'\n",
        "elif os.path.isfile(f'{trained_model_path}/pytorch_model.bin'):\n",
        "    model_path = trained_model_path\n",
        "\n",
        "print(f'Initialize Model from Path: {model_path}')\n",
        "translator = GermanSpeechToTextTranslater(model_name=model_path)"
      ],
      "id": "measured-craps",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initialize Model from Path: /content/gdrive/MyDrive/NLP-Models/GermanWave2Vec/trained_model_ah\n",
            "Using Model: /content/gdrive/MyDrive/NLP-Models/GermanWave2Vec/trained_model_ah\n",
            "Loading processor\n",
            "Loading metric\n",
            "Loading model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "internal-rider",
        "scrolled": false
      },
      "source": [
        "def train():\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=trained_model_path,\n",
        "      group_by_length=True,\n",
        "      per_device_train_batch_size=per_device_train_batch_size,\n",
        "      per_device_eval_batch_size=per_device_eval_batch_size, \n",
        "      gradient_accumulation_steps=gradient_accumulation_steps, \n",
        "      evaluation_strategy=\"steps\",\n",
        "      max_steps=max_steps,         \n",
        "      fp16=True,\n",
        "      save_steps=save_steps,     \n",
        "      eval_steps=eval_steps,     \n",
        "      logging_steps=logging_steps, \n",
        "      learning_rate=learning_rate,\n",
        "      warmup_steps=warmup_steps,   \n",
        "      save_total_limit=2\n",
        "  )\n",
        "\n",
        "  ds_to_train = []\n",
        "\n",
        "  for ds in get_used_datasets():\n",
        "      print(f'Use Dataset: {ds}')\n",
        "      ds_to_train.append(ds)\n",
        "      \n",
        "  calc_wer_after_training = False  # Wegen OutOfMemory ausgeschaltet \n",
        "\n",
        "  for runde in range(max_rounds):\n",
        "      print(f'Starting round {runde} of {max_rounds}')\n",
        "\n",
        "      for ds_id in ds_to_train:\n",
        "          # if not my_datasets.has_translation_with_original(ds_id):\n",
        "          #    print(f'Removing Dataset: {ds_id}')\n",
        "          #    ds_to_train.remove(ds_id)\n",
        "          #    continue\n",
        "\n",
        "          WANDB_NOTES=ds_id\n",
        "          stop = False\n",
        "          pandas_df = my_datasets.load_ds_content_translated_with_original(ds_id)\n",
        "          print(f'Dataset - {ds_id} loaded with {pandas_df.shape[0]} Entries')\n",
        "          mp3_dir = my_datasets.get_snippet_directory(ds_id)\n",
        "          \n",
        "          for epoche in range(num_train_epochs):\n",
        "              print(f'Starting round {runde} of {max_rounds}, epoche {epoche} of {num_train_epochs}')\n",
        "              print(f'Splitting Dataset {ds_id} with {pandas_df.shape[0]} Entries')\n",
        "              train_pandas_ds, test_pandas_ds = translator.split_dataset(\n",
        "                  pandas_df, \n",
        "                  max_trainingset_size=max_trainingset_size, \n",
        "                  max_sample_size=max_sample_size\n",
        "              )\n",
        "              \n",
        "              if train_pandas_ds.shape[0] > pandas_df.shape[0] * 0.02:            \n",
        "                  print(f'Creating Trainer for {ds_id}')\n",
        "                  trainer = translator.get_trainer(\n",
        "                      training_args, \n",
        "                      mp3_dir,             \n",
        "                      train_pandas_ds,\n",
        "                      test_pandas_ds,\n",
        "                      use_grouped_legth_trainer=False\n",
        "                  )\n",
        "                  print(f'Training of Dataset: {ds_id}')\n",
        "                  train_result = trainer.train()\n",
        "\n",
        "                  print(f'Save Model')\n",
        "                  trainer.save_model()\n",
        "                  metrics = train_result.metrics\n",
        "                  max_train_samples = train_pandas_ds.shape[0]\n",
        "                  metrics[\"train_samples\"] = min(max_train_samples, train_pandas_ds.shape[0])\n",
        "                  trainer.log_metrics(\"train\", metrics)\n",
        "                  trainer.save_metrics(\"train\", metrics)\n",
        "                  trainer.save_state()\n",
        "                  torch.cuda.empty_cache()\n",
        "\n",
        "                  last_checkpoint = get_last_checkpoint(trained_model_path)\n",
        "                  print( f'last Checkpoint: {last_checkpoint}')\n",
        "                  \n",
        "                  # print(f'Reload from last Checkpoint: {last_checkpoint}')\n",
        "                  # translator.reload_from_checkpoint(last_checkpoint)\n",
        "                  # {last_checkpoint}-\n",
        "                  bad_translation_ds, truncated_ds, wer_result = translator.test(\n",
        "                      ds_id, \n",
        "                      pandas_df, \n",
        "                      mp3_dir, \n",
        "                      diff_file_extension=f'{epoche}-{runde}', \n",
        "                      diff_calc_wer=calc_wer_after_training\n",
        "                  )\n",
        "                  \n",
        "                  if (bad_translation_ds.shape[0] < (truncated_ds.shape[0] * 0.02)) or (wer_result < 0.02):\n",
        "                      # mindestens 98% der Sätze wurde korrekt übersetzt. Überprüfung der Problemfälle ist angebracht.\n",
        "                      # Es hat sich gezeigt, dass das Ergebnis wieder schlechter werden kann.\n",
        "                      stop = True\n",
        "              else:\n",
        "                  stop = True\n",
        "\n",
        "              if stop:\n",
        "                  print('Early stopping!')\n",
        "                  ds_to_train.remove(ds_id)\n",
        "                  break\n",
        "\n",
        "  print('Training finisched!')"
      ],
      "id": "internal-rider",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHvPPiWru_IC"
      },
      "source": [
        "## 4. Choose Dataset"
      ],
      "id": "XHvPPiWru_IC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "2f9baed1d2f64d4492467605c4336ffc",
            "72ce045d20aa4ddebeaeb65e5067b799",
            "97e63375095b43b4a37e89b92646c69f",
            "5021dcab8b324849b30b5f4571b0d525",
            "93159f0b6985444e8c735261def3e951",
            "f1de6a11546e4c01801941129cb32109",
            "cd78cebf3e8e45feab11eb7476b0b51c",
            "f8eee87f3cf84b2aa0ddd8159c8c0e77",
            "f17e2066a1b743e88d2fe2d4f73db8bb",
            "f7e01e3e02c54cdf9dda32133db638a3",
            "86dc5c95694f436d86de86887f0fdc81",
            "e3ad19fff61149639b42ea1c6817e5f9",
            "dc4906791f5a4575842b627b90c298b2",
            "15ef26b6d3d2424d88278cafc7b90185",
            "1d41f5c7909c46ba87ccb185b3756fb0",
            "46ed4f6fe3d249879807afaf62f56d7d",
            "acaadf06c3d54139acac2fead50f0acf",
            "0cc2d2a5c9354902ab7df80c2ccb810c",
            "c75bd938352c499ab58fc7c553119d8e",
            "94f6823155e34ef0907f4e6c97346ccd",
            "d186d3bfe18b44d68e060cacc959e076",
            "97afa66d1c854a1b8fe1511a580b20cf",
            "6e02e1aac6dc46a58fc123cc4589187d"
          ]
        },
        "id": "A2FODbnsFmoC",
        "outputId": "e0f3c9ea-596d-4227-a6e6-ecdba3d6f9eb"
      },
      "source": [
        "all_ds_ids = set()\n",
        "all_ds_ids.update(list(my_datasets.local_datasets.keys()))\n",
        "all_ds_ids.update(list(my_datasets.extern_datasets.keys()))\n",
        "\n",
        "import difflib\n",
        "from functools import partial\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Audio\n",
        "\n",
        "ds_checkbox_items = []\n",
        "\n",
        "for ds_id in sorted(all_ds_ids):\n",
        "    ds_checkbox_items.append(widgets.Checkbox(\n",
        "        value=ds_id in ds_to_us, \n",
        "        description=ds_id, \n",
        "        disabled=False, \n",
        "        indent=False\n",
        "    ))\n",
        "\n",
        "def get_used_datasets(checkbox_items=ds_checkbox_items):\n",
        "    result = []\n",
        "\n",
        "    for item in checkbox_items:\n",
        "        if item.value:\n",
        "            result.append(item.description)\n",
        "\n",
        "    return result\n",
        "\n",
        "print('Please choose Datasets to use')\n",
        "ds_checkboxes_widget = widgets.Box(ds_checkbox_items)\n",
        "display(ds_checkboxes_widget)"
      ],
      "id": "A2FODbnsFmoC",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please choose Datasets to use\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f9baed1d2f64d4492467605c4336ffc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Box(children=(Checkbox(value=True, description='HP1-FvM', indent=False), Checkbox(value=False, description='HP…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWEV0YTpIxw3",
        "outputId": "8ceba724-ef57-45af-e146-45c2565b695a"
      },
      "source": [
        "train()"
      ],
      "id": "qWEV0YTpIxw3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use Dataset: HP1-FvM\n",
            "Starting round 0 of 1\n",
            "Loading Dataset: HP1-FvM - content-translated-with_original.csv\n",
            "Pruning Dataset HP1-FvM with 8055 Entries\n",
            " - 8055 Entries left after Length Cut (min=31, max=4000)\n",
            " - 8055 Entries left after Action Cut\n",
            "Dataset - HP1-FvM loaded with 8055 Entries\n",
            "Starting round 0 of 1, epoche 0 of 1\n",
            "Splitting Dataset HP1-FvM with 8055 Entries\n",
            " - 8055 Entries left after Action Cut\n",
            " - 8055 Entries left after Length Cut (max=1200)\n",
            " - 8055 Entries left after minimal Size Cut (min=31)\n",
            "Training Dataset Size: 7855, Validation Dataset Size 200\n",
            " - 3782 Entries left after first Recognition Cut\n",
            " - 2500 left after Entries Max Samples Cut (max=2500)\n",
            "Training Dataset Size: 2500, Validation Dataset Size 200\n",
            "Creating Trainer for HP1-FvM\n",
            "Training of Dataset: HP1-FvM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrLdZDzVthSj"
      },
      "source": [
        "def on_train_action_button_clicked(b):\n",
        "    print('Train')\n",
        "    train()\n",
        "\n",
        "def on_translate_action_button_clicked(b):\n",
        "    print('Translate')\n",
        "    for ds_id in get_used_datasets():\n",
        "        translator.translate_and_extend_dataset_from_directory(my_datasets, ds_id)    \n",
        "\n",
        "class InvisibleAudio(Audio):\n",
        "    def _repr_html_(self):\n",
        "        audio = super()._repr_html_()\n",
        "        audio = audio.replace('<audio', f'<audio onended=\"this.parentNode.removeChild(this)\"')\n",
        "        return f'<div style=\"display:none\">{audio}</div>'\n",
        "\n",
        "def show_diff(text, n_text):\n",
        "    \"\"\"\n",
        "    http://stackoverflow.com/a/788780\n",
        "    Unify operations between two compared strings seqm is a difflib.\n",
        "    SequenceMatcher instance whose a & b are strings\n",
        "    \"\"\"\n",
        "    seqm = difflib.SequenceMatcher(None, text, n_text)\n",
        "    output= []\n",
        "    for opcode, a0, a1, b0, b1 in seqm.get_opcodes():\n",
        "        if opcode == 'equal':\n",
        "            output.append(seqm.a[a0:a1])\n",
        "        elif opcode == 'insert':\n",
        "            output.append(\"<font color=red>^\" + seqm.b[b0:b1] + \"</font>\")\n",
        "        elif opcode == 'delete':\n",
        "            output.append(\"<font color=blue>^\" + seqm.a[a0:a1] + \"</font>\")\n",
        "        elif opcode == 'replace':\n",
        "            # seqm.a[a0:a1] -> seqm.b[b0:b1]\n",
        "            output.append(\"<font color=green>^\" + seqm.b[b0:b1] + \"</font>\")\n",
        "        else:\n",
        "            raise RuntimeError(\"unexpected opcode\")\n",
        "    return ''.join(output)\n",
        "    \n",
        "def create_diff_row(audio_file, original_text, translated_text ):\n",
        "    diff_widget = widgets.HTML(value=show_diff(original_text, translated_text))\n",
        "    button_widget = widgets.Button(description='Play')\n",
        "\n",
        "    def on_play_button_clicked(f, b):\n",
        "        display(InvisibleAudio(url=audio_file, autoplay=True))\n",
        "\n",
        "    button_widget.on_click(partial(on_play_button_clicked, 1))\n",
        "    return widgets.HBox([button_widget, diff_widget])\n",
        "\n",
        "def create_diff_content(ds_id):\n",
        "    ds = my_datasets.load_ds_content_translated_with_original(ds_id)\n",
        "    snipped_directory = my_datasets.get_snippet_directory(ds_id)\n",
        "    print(f'Use Snipped Directory: {snipped_directory}')\n",
        "    translation_row = 'Translated1' if 'Translated1' in ds.columns else 'Translated0'\n",
        "    ds = ds[ds['OriginalText'] != ds[translation_row]]\n",
        "    rows = []\n",
        "    max_len = min(20, len(ds))\n",
        "\n",
        "    for idx in tqdm_notebook(range(max_len)):\n",
        "        rows.append(create_diff_row(\n",
        "            f'{snipped_directory}/{ds.iloc[idx][\"Datei\"]}', \n",
        "            ds.iloc[idx]['OriginalText'], \n",
        "            ds.iloc[idx][translation_row]\n",
        "        ))\n",
        "\n",
        "    return widgets.VBox(rows)\n",
        "\n",
        "def on_validate_action_button_clicked(b):\n",
        "    print('Validate')\n",
        "    tab = widgets.Tab()\n",
        "    tab_titles = []\n",
        "    tab_children = []\n",
        "\n",
        "    for ds_id in get_used_datasets():\n",
        "        tab_titles.append(ds_id)\n",
        "        tab_children.append(create_diff_content(ds_id))\n",
        "\n",
        "    tab.children = tab_children\n",
        "  \n",
        "    for idx, name in enumerate(tab_titles):\n",
        "        tab.set_title(idx, name)\n",
        "  \n",
        "    display(tab)\n",
        "\n",
        "\n",
        "train_action_button = widgets.Button(description='Train')\n",
        "train_action_button.on_click(on_train_action_button_clicked)\n",
        "translate_action_button = widgets.Button(description='Translate')\n",
        "translate_action_button.on_click(on_translate_action_button_clicked)\n",
        "validate_action_button = widgets.Button(description='Validate')\n",
        "validate_action_button.on_click(on_validate_action_button_clicked)\n",
        "\n",
        "action_buttons = widgets.HBox([\n",
        "    train_action_button, \n",
        "    translate_action_button, \n",
        "    validate_action_button\n",
        "])\n",
        "display(action_buttons)"
      ],
      "id": "yrLdZDzVthSj",
      "execution_count": null,
      "outputs": []
    }
  ]
}